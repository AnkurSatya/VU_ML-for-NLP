{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic System\n",
    "\n",
    "This notebook provides code for implementing a very simple machine learning system for named entity recognition.\n",
    "It uses logistic regression and one feature (the token itself).\n",
    "Links to information about the packages are provided. Your job is to document the code and use it to train a system. You can then use your evaluation code to provide the first basic evaluation of your system.\n",
    "In the next assignment, you can use this as a basis to experiment with more features and more machine learning methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import pandas as pd\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "\n",
    "# If you want to include other modules, you can add them here\n",
    "# Please note the recommendations on using modules in the Programming General Guidelines\n",
    "\n",
    "#recommended resource for examples:\n",
    "\n",
    "#https://scikit-learn.org/stable/modules/feature_extraction.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_capital(word):\n",
    "    return int(bool(re.search(\"^[A-Z]\", word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_and_labels(trainingfile):\n",
    "    \n",
    "    data = []\n",
    "    targets = []\n",
    "    with open(trainingfile, 'r', encoding='utf8') as infile:\n",
    "        is_prev_word_period = True # set to true so that the first word of the file is considered for\n",
    "        # being 'the first word' check.\n",
    "        for line in infile:\n",
    "            components = line.rstrip('\\n').split()\n",
    "            if len(components) > 0:\n",
    "                \n",
    "                is_first_word = False\n",
    "                if is_prev_word_period and not bool(re.search('^[_\\W]*?$', components[0])):\n",
    "                    is_first_word = True\n",
    "                    \n",
    "                token, pos = components[0:2]\n",
    "                is_word_capital = is_capital(components[0])\n",
    "                feature_dict = {'token':token, 'pos': pos, 'is_first_word': is_first_word, \n",
    "                                'is_capital': is_word_capital}\n",
    "                data.append(feature_dict)\n",
    "                #gold is in the last column\n",
    "                targets.append(components[-1])\n",
    "                \n",
    "                is_prev_word_period = bool(components[0] == '.') and bool(components[-1] == 'O')\n",
    "                    \n",
    "    return data, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(inputfile):\n",
    "   \n",
    "    data = []\n",
    "    with open(inputfile, 'r', encoding='utf8') as infile:\n",
    "        is_prev_word_period = True # set to true so that the first word of the file is considered for\n",
    "        # being 'the first word' check.\n",
    "        for line in infile:\n",
    "            components = line.rstrip('\\n').split()\n",
    "            if len(components) > 0:\n",
    "                \n",
    "                is_first_word = False\n",
    "                if is_prev_word_period and not bool(re.search('^[_\\W]*?$', components[0])):\n",
    "                    is_first_word = True\n",
    "                    \n",
    "                token, pos = components[0:2]\n",
    "                is_word_capital = is_capital(components[0])\n",
    "                feature_dict = {'token':token, 'pos': pos, 'is_first_word': is_first_word, \n",
    "                                'is_capital': is_word_capital}\n",
    "                data.append(feature_dict)\n",
    "                is_prev_word_period = bool(components[0] == '.') and bool(components[-1] == 'O')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier(train_features, train_targets):\n",
    "   \n",
    "    logreg = LogisticRegression()\n",
    "    vec = DictVectorizer()\n",
    "    features_vectorized = vec.fit_transform(train_features)\n",
    "    print(features_vectorized.shape)\n",
    "    print(vec.get_feature_names_out())\n",
    "    model = logreg.fit(features_vectorized, train_targets)\n",
    "    \n",
    "    return model, vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_data(model, vec, inputdata, outputfile):\n",
    "  \n",
    "    features = extract_features(inputdata)\n",
    "    features = vec.transform(features)\n",
    "    predictions = model.predict(features)\n",
    "    outfile = open(outputfile, 'w')\n",
    "    counter = 0\n",
    "    for line in open(inputdata, 'r'):\n",
    "        if len(line.rstrip('\\n').split()) > 0:\n",
    "            outfile.write(line.rstrip('\\n') + '\\t' + predictions[counter] + '\\n')\n",
    "            counter += 1\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(argv=None):\n",
    "    \n",
    "    #a very basic way for picking up commandline arguments\n",
    "    if argv is None:\n",
    "        argv = sys.argv\n",
    "        \n",
    "    #Note 1: argv[0] is the name of the python program if you run your program as: python program1.py arg1 arg2 arg3\n",
    "    #Note 2: sys.argv is simple, but gets messy if you need it for anything else than basic scenarios with few arguments\n",
    "    #you'll want to move to something better. e.g. argparse (easy to find online)\n",
    "    \n",
    "    \n",
    "    #you can replace the values for these with paths to the appropriate files for now, e.g. by specifying values in argv\n",
    "    #argv = ['mypython_program','','','']\n",
    "    trainingfile = argv[1]\n",
    "    inputfile = argv[2]\n",
    "    outputfile = argv[3]\n",
    "    \n",
    "    training_features, gold_labels = extract_features_and_labels(trainingfile)\n",
    "    \n",
    "    ml_model, vec = create_classifier(training_features, gold_labels)\n",
    "#     classify_data(ml_model, vec, inputfile, outputfile)\n",
    "\n",
    "# uncomment this when using this in a script    \n",
    "    \n",
    "#if __name__ == '__main__':\n",
    "#    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(203621, 23670)\n",
      "['is_capital' 'is_first_word' 'pos=\"' ... 'token=zlotys' 'token=zone'\n",
      " 'token=zvezda']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankur/miniconda3/envs/env_ml-for-nlp/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# remember that the first element of the list is not used \n",
    "# (since this is the `python command when the args are read from sys.argv)\n",
    "# make sure to complete the rest of the list assigned to args correctly\n",
    "args = ['python', \"../../data/conll2003.train.conll\", \"../../data/conll2003.dev.conll\", \"prediction.txt\"]\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ml-for-nlp",
   "language": "python",
   "name": "env_ml-for-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
